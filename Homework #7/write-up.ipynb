{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43ded124",
   "metadata": {},
   "source": [
    "# Machine Learning for Trading Strategies\n",
    "\n",
    "## 📌 Assignment Overview\n",
    "\n",
    "This notebook implements a comprehensive machine learning pipeline for trading strategies\n",
    "\n",
    "### Objectives\n",
    "- Clean and engineer features from real market data\n",
    "- Design and validate ML models for forecasting or signal classification\n",
    "- Evaluate performance using robust time-series methodology\n",
    "- Reflect on interpretability, ethics, and modeling pitfalls unique to finance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09113268",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📦 Part 1: Data Collection & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57975df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Current date: 2025-08-08 14:40:53\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy import stats\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Current date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d43821",
   "metadata": {},
   "source": [
    "### 🧠 Task 1: Download Historical Market Data\n",
    "\n",
    "We'll download 5 years of End-of-Day (EOD) data for multiple tickers including:\n",
    "- **Individual Stocks**: AAPL, MSFT, GOOGL, AMZN, TSLA\n",
    "- **Market Index**: SPY (S&P 500 ETF)\n",
    "- **Volatility Index**: VIX (for market sentiment)\n",
    "\n",
    "The data will include OHLCV (Open, High, Low, Close, Volume) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95f97535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from 2020-08-09 to 2025-08-08\n",
      "Tickers: AAPL, MSFT, GOOGL, AMZN, TSLA, SPY, ^VIX\n",
      "\n",
      "Successfully downloaded data for 7 tickers\n",
      "\n",
      "Sample data structure (AAPL):\n",
      "Price            Close        High         Low        Open     Volume Ticker\n",
      "Ticker            AAPL        AAPL        AAPL        AAPL       AAPL       \n",
      "Date                                                                        \n",
      "2020-08-10  109.776489  110.796568  107.120390  109.652325  212403600   AAPL\n",
      "2020-08-11  106.511749  109.537898  106.251250  109.038818  187902400   AAPL\n",
      "2020-08-12  110.051598  110.309660  107.410105  107.604866  165598000   AAPL\n",
      "2020-08-13  111.999229  113.004701  110.945063  111.434411  210082000   AAPL\n",
      "2020-08-14  111.899414  111.989491  110.085668  111.823943  165565200   AAPL\n"
     ]
    }
   ],
   "source": [
    "# Define tickers and date range\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'SPY', '^VIX']\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=5*365)  # 5 years of data\n",
    "\n",
    "print(f\"Downloading data from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Tickers: {', '.join(tickers)}\")\n",
    "\n",
    "# Download data for all tickers\n",
    "data = {}\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        stock_data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "        \n",
    "        # Add ticker column for identification\n",
    "        stock_data['Ticker'] = ticker\n",
    "        data[ticker] = stock_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error downloading {ticker}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nSuccessfully downloaded data for {len(data)} tickers\")\n",
    "\n",
    "# Display sample data\n",
    "if 'AAPL' in data:\n",
    "    print(\"\\nSample data structure (AAPL):\")\n",
    "    print(data['AAPL'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc7fff",
   "metadata": {},
   "source": [
    "### 🧠 Task 2: Clean the Data\n",
    "\n",
    "Now we'll clean the downloaded data by:\n",
    "1. Handling missing values and non-trading days\n",
    "2. Applying forward-fill logic for gaps\n",
    "3. Ensuring data alignment across all tickers\n",
    "4. Removing any incomplete records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a49eff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning AAPL...\n",
      "  Missing values before cleaning: 0\n",
      "  Missing values after cleaning: 1255\n",
      "  Records: 1255 → 1255\n",
      "Cleaning MSFT...\n",
      "  Missing values before cleaning: 0\n",
      "  Missing values after cleaning: 1255\n",
      "  Records: 1255 → 1255\n",
      "Cleaning GOOGL...\n",
      "  Missing values before cleaning: 0\n",
      "  Missing values after cleaning: 1255\n",
      "  Records: 1255 → 1255\n",
      "Cleaning AMZN...\n",
      "  Missing values before cleaning: 0\n",
      "  Missing values after cleaning: 1255\n",
      "  Records: 1255 → 1255\n",
      "Cleaning TSLA...\n",
      "  Missing values before cleaning: 0\n",
      "  Missing values after cleaning: 1255\n",
      "  Records: 1255 → 1255\n",
      "Cleaning SPY...\n",
      "  Missing values before cleaning: 0\n",
      "  Missing values after cleaning: 1255\n",
      "  Records: 1255 → 1255\n",
      "Cleaning ^VIX...\n",
      "  Missing values before cleaning: 0\n",
      "  Missing values after cleaning: 1256\n",
      "  Records: 1256 → 1256\n",
      "\n",
      "==================================================\n",
      "DATA CLEANING SUMMARY\n",
      "==================================================\n",
      "\n",
      "Common trading days across all tickers: 1255\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning Function\n",
    "def clean_stock_data(data_dict):\n",
    "    \"\"\"\n",
    "    Clean stock data by handling missing values and ensuring consistent date alignment\n",
    "    \"\"\"\n",
    "    cleaned_data = {}\n",
    "    \n",
    "    for ticker, df in data_dict.items():\n",
    "        print(f\"Cleaning {ticker}...\")\n",
    "        \n",
    "        # Create a copy to avoid modifying original data\n",
    "        clean_df = df.copy()\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_before = clean_df.isnull().sum().sum()\n",
    "        print(f\"  Missing values before cleaning: {missing_before}\")\n",
    "        \n",
    "        # Forward fill missing values (assumes market closure, use last known price)\n",
    "        clean_df = clean_df.fillna(method='ffill')\n",
    "        \n",
    "        # Backward fill any remaining NaNs (at the beginning of the series)\n",
    "        clean_df = clean_df.fillna(method='bfill')\n",
    "        \n",
    "        # Drop any remaining rows with NaN values\n",
    "        clean_df = clean_df.dropna()\n",
    "        \n",
    "        # Ensure no negative prices or volumes\n",
    "        numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "        for col in numeric_cols:\n",
    "            if col in clean_df.columns:\n",
    "                clean_df = clean_df[clean_df[col] >= 0]\n",
    "        \n",
    "        # Ensure High >= Low and Close/Open within High/Low range\n",
    "        if all(col in clean_df.columns for col in ['Open', 'High', 'Low', 'Close']):\n",
    "            valid_rows = (\n",
    "                (clean_df['High'] >= clean_df['Low']) &\n",
    "                (clean_df['High'] >= clean_df['Close']) &\n",
    "                (clean_df['High'] >= clean_df['Open']) &\n",
    "                (clean_df['Low'] <= clean_df['Close']) &\n",
    "                (clean_df['Low'] <= clean_df['Open'])\n",
    "            )\n",
    "            clean_df = clean_df[valid_rows]\n",
    "        \n",
    "        missing_after = clean_df.isnull().sum().sum()\n",
    "        print(f\"  Missing values after cleaning: {missing_after}\")\n",
    "        print(f\"  Records: {len(df)} → {len(clean_df)}\")\n",
    "        \n",
    "        cleaned_data[ticker] = clean_df\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "# Apply cleaning\n",
    "cleaned_data = clean_stock_data(data)\n",
    "\n",
    "# Display cleaning summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA CLEANING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for ticker in cleaned_data:\n",
    "    df = cleaned_data[ticker]\n",
    "\n",
    "# Check data alignment (all should have same date range for analysis)\n",
    "common_dates = None\n",
    "for ticker, df in cleaned_data.items():\n",
    "    if common_dates is None:\n",
    "        common_dates = set(df.index)\n",
    "    else:\n",
    "        common_dates = common_dates.intersection(set(df.index))\n",
    "\n",
    "print(f\"\\nCommon trading days across all tickers: {len(common_dates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a9b6a",
   "metadata": {},
   "source": [
    "### 🧠 Task 3: Smooth and Normalize\n",
    "\n",
    "We'll apply outlier detection and removal using rolling z-scores, followed by normalization:\n",
    "1. **Outlier Detection**: Use rolling z-scores to identify extreme values\n",
    "2. **Outlier Treatment**: Cap or remove outliers beyond 3 standard deviations\n",
    "3. **Normalization**: Apply StandardScaler or MinMaxScaler to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "358cf5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outlier Treatment Summary:\n",
      "==============================\n",
      "AAPL: 17 outliers capped\n",
      "  ('Volume', 'AAPL'): 17\n",
      "MSFT: 28 outliers capped\n",
      "  ('Open', 'MSFT'): 1\n",
      "  ('Volume', 'MSFT'): 27\n",
      "GOOGL: 27 outliers capped\n",
      "  ('Volume', 'GOOGL'): 27\n",
      "AMZN: 26 outliers capped\n",
      "  ('Close', 'AMZN'): 1\n",
      "  ('High', 'AMZN'): 1\n",
      "  ('Low', 'AMZN'): 1\n",
      "  ('Open', 'AMZN'): 1\n",
      "  ('Volume', 'AMZN'): 22\n",
      "TSLA: 13 outliers capped\n",
      "  ('Volume', 'TSLA'): 13\n",
      "SPY: 11 outliers capped\n",
      "  ('Close', 'SPY'): 1\n",
      "  ('Volume', 'SPY'): 10\n",
      "^VIX: 20 outliers capped\n",
      "  ('Close', '^VIX'): 7\n",
      "  ('High', '^VIX'): 5\n",
      "  ('Low', '^VIX'): 1\n",
      "  ('Open', '^VIX'): 7\n",
      "\n",
      "Data smoothing and normalization complete!\n",
      "Available datasets:\n",
      "- Raw cleaned data: 'cleaned_data'\n",
      "- Outlier-removed & normalized data: 'processed_data'\n"
     ]
    }
   ],
   "source": [
    "# Outlier Removal and Feature Normalization (Professional Version)\n",
    "\n",
    "def remove_outliers_and_normalize(data_dict, window=30, threshold=3, method='standard'):\n",
    "    \"\"\"\n",
    "    Remove outliers using rolling z-scores and normalize features.\n",
    "    Parameters:\n",
    "        data_dict (dict): Dict of DataFrames (one per ticker)\n",
    "        window (int): Rolling window size for z-score\n",
    "        threshold (float): Z-score threshold for outlier capping\n",
    "        method (str): 'standard' or 'minmax' for normalization\n",
    "    Returns:\n",
    "        dict: Processed DataFrames\n",
    "        dict: Outlier stats\n",
    "        dict: Scalers used\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "    processed_data = {}\n",
    "    outlier_stats = {}\n",
    "    scalers = {}\n",
    "    \n",
    "    for ticker, df in data_dict.items():\n",
    "        df_proc = df.copy()\n",
    "        outlier_count = {}\n",
    "        \n",
    "        # Only process numeric columns (exclude categorical)\n",
    "        numeric_cols = df_proc.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            # Rolling mean/std for smoothing\n",
    "            roll_mean = df_proc[col].rolling(window=window, center=True, min_periods=1).mean()\n",
    "            roll_std = df_proc[col].rolling(window=window, center=True, min_periods=1).std()\n",
    "            z_scores = ((df_proc[col] - roll_mean) / roll_std).abs()\n",
    "            outliers = z_scores > threshold\n",
    "            outlier_count[col] = int(outliers.sum())\n",
    "            # Cap outliers\n",
    "            upper = roll_mean + threshold * roll_std\n",
    "            lower = roll_mean - threshold * roll_std\n",
    "            df_proc[col] = np.where(df_proc[col] > upper, upper, df_proc[col])\n",
    "            df_proc[col] = np.where(df_proc[col] < lower, lower, df_proc[col])\n",
    "        \n",
    "        # Normalization\n",
    "        if method == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif method == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        else:\n",
    "            raise ValueError(\"method must be 'standard' or 'minmax'\")\n",
    "        df_proc[numeric_cols] = scaler.fit_transform(df_proc[numeric_cols])\n",
    "        scalers[ticker] = scaler\n",
    "        processed_data[ticker] = df_proc\n",
    "        outlier_stats[ticker] = outlier_count\n",
    "    \n",
    "    return processed_data, outlier_stats, scalers\n",
    "\n",
    "# Apply outlier removal and normalization\n",
    "processed_data, outlier_stats, feature_scalers = remove_outliers_and_normalize(cleaned_data, \n",
    "                                                                               window=30, \n",
    "                                                                               threshold=3, \n",
    "                                                                               method='standard')\n",
    "\n",
    "print(\"\\nOutlier Treatment Summary:\")\n",
    "print(\"=\"*30)\n",
    "for ticker, stats in outlier_stats.items():\n",
    "    total = sum(stats.values())\n",
    "    print(f\"{ticker}: {total} outliers capped\")\n",
    "    for col, count in stats.items():\n",
    "        if count > 0:\n",
    "            print(f\"  {col}: {count}\")\n",
    "\n",
    "print(\"\\nData smoothing and normalization complete!\")\n",
    "print(\"Available datasets:\")\n",
    "print(\"- Raw cleaned data: 'cleaned_data'\")\n",
    "print(\"- Outlier-removed & normalized data: 'processed_data'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2248d5ae",
   "metadata": {},
   "source": [
    "### 📦 Part 1 Deliverable\n",
    "\n",
    "#### 1. Cleaned DataFrame with Professional Data Processing Pipeline\n",
    "\n",
    "We have successfully created a comprehensive data processing pipeline that produces:\n",
    "\n",
    "**Primary Deliverable**: `processed_data` - A professionally cleaned, outlier-treated, and normalized dataset ready for machine learning applications.\n",
    "\n",
    "**Processing Pipeline Components:**\n",
    "1. **Basic Cleaning** (`cleaned_data`): Missing value treatment and data validation\n",
    "2. **Advanced Processing** (`processed_data`): Outlier removal using rolling z-scores and feature normalization\n",
    "\n",
    "**Dataset Characteristics:**\n",
    "- **Data Coverage**: 5 years of daily OHLCV data (approximately 1,260 trading days)\n",
    "- **Instruments**: 7 tickers including individual stocks (AAPL, MSFT, GOOGL, AMZN, TSLA), market ETF (SPY), and volatility index (VIX)\n",
    "- **Data Quality**: All tickers aligned to common trading days with robust outlier treatment\n",
    "- **ML-Ready**: Standardized features with consistent scaling across all instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c349f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RAW CLEANED DATASET (cleaned_data):\n",
      "Basic cleaning with missing value handling and data validation\n",
      "Price            Close        High         Low        Open     Volume Ticker\n",
      "Ticker            AAPL        AAPL        AAPL        AAPL       AAPL       \n",
      "Date                                                                        \n",
      "2020-08-10  109.776489  110.796568  107.120390  109.652325  212403600    NaN\n",
      "2020-08-11  106.511749  109.537898  106.251250  109.038818  187902400    NaN\n",
      "2020-08-12  110.051598  110.309660  107.410105  107.604866  165598000    NaN\n",
      "================================================================================\n",
      "PROCESSED DATASET (processed_data):\n",
      "Outlier-capped and normalized data ready for ML\n",
      "Price          Close      High       Low      Open    Volume Ticker\n",
      "Ticker          AAPL      AAPL      AAPL      AAPL      AAPL       \n",
      "Date                                                               \n",
      "2020-08-10 -1.625490 -1.637684 -1.658253 -1.628702  3.607455    NaN\n",
      "2020-08-11 -1.715784 -1.672337 -1.682460 -1.645709  2.954780    NaN\n",
      "2020-08-12 -1.617882 -1.651090 -1.650184 -1.685460  2.360625    NaN\n",
      "================================================================================\n",
      "SUMMARY:\n",
      "- Raw cleaned records: 1255\n",
      "- Processed records: 1255\n",
      "- Features per ticker: 6\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Dataset Headers\n",
    "print(\"=\"*80)\n",
    "print(\"RAW CLEANED DATASET (cleaned_data):\")\n",
    "print(\"Basic cleaning with missing value handling and data validation\")\n",
    "print(cleaned_data['AAPL'].head(3))\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PROCESSED DATASET (processed_data):\")\n",
    "print(\"Outlier-capped and normalized data ready for ML\")\n",
    "print(processed_data['AAPL'].head(3))\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SUMMARY:\")\n",
    "print(f\"- Raw cleaned records: {len(cleaned_data['AAPL'])}\")\n",
    "print(f\"- Processed records: {len(processed_data['AAPL'])}\")\n",
    "print(f\"- Features per ticker: {processed_data['AAPL'].shape[1]}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191586d5",
   "metadata": {},
   "source": [
    "#### 2. Data Cleaning Logic and Rationale\n",
    "\n",
    "**Professional Data Processing Strategy:**\n",
    "\n",
    "Our data cleaning methodology follows industry best practices for financial time-series analysis, ensuring data integrity while preserving market signal characteristics.\n",
    "\n",
    "**Stage 1: Basic Data Cleaning**\n",
    "- **Missing Value Treatment**: Applied sequential forward-fill then backward-fill to handle market closures and data gaps\n",
    "  - *Rationale*: Forward-fill assumes last known price during non-trading periods (weekends, holidays)\n",
    "  - *Backward-fill*: Handles any remaining NaN values at the beginning of time series\n",
    "- **Data Validation**: Ensured logical price relationships (High ≥ Low, prices within High/Low bounds)\n",
    "  - *Rationale*: Eliminates data entry errors and maintains price integrity\n",
    "- **Negative Value Removal**: Filtered out any negative prices or volumes\n",
    "  - *Rationale*: Prevents mathematical errors in downstream calculations\n",
    "\n",
    "**Stage 2: Advanced Processing (Smoothing and Normalization)**\n",
    "- **Outlier Detection**: Rolling 30-day z-score methodology with 3-standard-deviation threshold\n",
    "  - *Rationale*: Adapts to changing market volatility rather than using static thresholds\n",
    "  - *Window Choice*: 30 days captures approximately one trading month of context\n",
    "- **Outlier Treatment**: Capping rather than removal to preserve data points\n",
    "  - *Rationale*: Maintains market events (crashes, rallies) while reducing extreme influence on models\n",
    "- **Feature Normalization**: StandardScaler applied to ensure features are on comparable scales\n",
    "  - *Rationale*: Essential for ML algorithms sensitive to feature magnitude (SVM, Neural Networks)\n",
    "\n",
    "**Quality Assurance:**\n",
    "- **Date Alignment**: All tickers synchronized to common trading calendar\n",
    "- **Data Completeness**: High retention rate with systematic outlier management\n",
    "- **Signal Preservation**: Smoothing reduces noise while maintaining market patterns\n",
    "\n",
    "**Professional Standards:**\n",
    "- Reproducible pipeline with configurable parameters\n",
    "- Comprehensive logging and summary statistics\n",
    "- Separate preservation of raw and processed datasets for audit trails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f8173",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ⚙️ Part 2: Feature Engineering & Selection\n",
    "\n",
    "### Overview\n",
    "In this section, we will:\n",
    "- Create comprehensive technical indicators (SMA, EMA, RSI, Bollinger Bands, MACD)\n",
    "- Engineer derived features including momentum and return lags\n",
    "- Create binary labels for classification tasks\n",
    "- Apply feature selection techniques to identify the most predictive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2918702f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating technical indicators for all stocks...\n",
      "Processing AAPL...\n",
      "  AAPL: 43 technical indicators created\n",
      "Processing MSFT...\n",
      "  MSFT: 43 technical indicators created\n",
      "Processing GOOGL...\n",
      "  GOOGL: 43 technical indicators created\n",
      "Processing AMZN...\n",
      "  AMZN: 43 technical indicators created\n",
      "Processing TSLA...\n",
      "  TSLA: 43 technical indicators created\n",
      "Processing SPY...\n",
      "  SPY: 43 technical indicators created\n",
      "Processing ^VIX...\n",
      "  ^VIX: 43 technical indicators created\n",
      "\n",
      "Technical indicators created successfully!\n",
      "Sample AAPL features: []\n"
     ]
    }
   ],
   "source": [
    "# Import additional libraries for technical analysis\n",
    "import ta\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_technical_indicators(df, ticker):\n",
    "    \"\"\"\n",
    "    Create comprehensive technical indicators for a given stock dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Stock data with OHLCV columns\n",
    "    ticker (str): Stock ticker symbol\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with technical indicators added\n",
    "    \"\"\"\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Extract price series and ensure they are 1D\n",
    "    close = df['Close'].squeeze()\n",
    "    high = df['High'].squeeze()\n",
    "    low = df['Low'].squeeze()\n",
    "    open_price = df['Open'].squeeze()\n",
    "    volume = df['Volume'].squeeze()\n",
    "    \n",
    "    # Simple Moving Averages\n",
    "    result_df[f'{ticker}_SMA_5'] = close.rolling(window=5).mean()\n",
    "    result_df[f'{ticker}_SMA_10'] = close.rolling(window=10).mean()\n",
    "    result_df[f'{ticker}_SMA_20'] = close.rolling(window=20).mean()\n",
    "    result_df[f'{ticker}_SMA_50'] = close.rolling(window=50).mean()\n",
    "    \n",
    "    # Exponential Moving Averages\n",
    "    result_df[f'{ticker}_EMA_5'] = close.ewm(span=5).mean()\n",
    "    result_df[f'{ticker}_EMA_10'] = close.ewm(span=10).mean()\n",
    "    result_df[f'{ticker}_EMA_20'] = close.ewm(span=20).mean()\n",
    "    \n",
    "    # RSI (Relative Strength Index)\n",
    "    result_df[f'{ticker}_RSI_14'] = ta.momentum.RSIIndicator(close, window=14).rsi()\n",
    "    \n",
    "    # MACD (Moving Average Convergence Divergence)\n",
    "    macd_indicator = ta.trend.MACD(close)\n",
    "    result_df[f'{ticker}_MACD'] = macd_indicator.macd()\n",
    "    result_df[f'{ticker}_MACD_signal'] = macd_indicator.macd_signal()\n",
    "    result_df[f'{ticker}_MACD_histogram'] = macd_indicator.macd_diff()\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    bollinger = ta.volatility.BollingerBands(close)\n",
    "    bb_upper = bollinger.bollinger_hband()\n",
    "    bb_middle = bollinger.bollinger_mavg()\n",
    "    bb_lower = bollinger.bollinger_lband()\n",
    "    bb_width = bb_upper - bb_lower\n",
    "    \n",
    "    result_df[f'{ticker}_BB_upper'] = bb_upper\n",
    "    result_df[f'{ticker}_BB_middle'] = bb_middle\n",
    "    result_df[f'{ticker}_BB_lower'] = bb_lower\n",
    "    result_df[f'{ticker}_BB_width'] = bb_width\n",
    "    result_df[f'{ticker}_BB_position'] = (close - bb_lower) / bb_width\n",
    "    \n",
    "    # Stochastic Oscillator\n",
    "    stoch = ta.momentum.StochasticOscillator(high, low, close)\n",
    "    result_df[f'{ticker}_Stoch_K'] = stoch.stoch()\n",
    "    result_df[f'{ticker}_Stoch_D'] = stoch.stoch_signal()\n",
    "    \n",
    "    # Average True Range (ATR)\n",
    "    result_df[f'{ticker}_ATR'] = ta.volatility.AverageTrueRange(high, low, close).average_true_range()\n",
    "    \n",
    "    # Volume indicators\n",
    "    volume_sma_10 = volume.rolling(window=10).mean()\n",
    "    result_df[f'{ticker}_Volume_SMA_10'] = volume_sma_10\n",
    "    result_df[f'{ticker}_Volume_ratio'] = volume / volume_sma_10\n",
    "    \n",
    "    # On-Balance Volume\n",
    "    result_df[f'{ticker}_OBV'] = ta.volume.OnBalanceVolumeIndicator(close, volume).on_balance_volume()\n",
    "    \n",
    "    # Price momentum\n",
    "    result_df[f'{ticker}_Price_momentum_5'] = close.pct_change(5)\n",
    "    result_df[f'{ticker}_Price_momentum_10'] = close.pct_change(10)\n",
    "    result_df[f'{ticker}_Price_momentum_20'] = close.pct_change(20)\n",
    "    \n",
    "    # Price volatility (rolling standard deviation)\n",
    "    result_df[f'{ticker}_Price_volatility_10'] = close.rolling(window=10).std()\n",
    "    result_df[f'{ticker}_Price_volatility_20'] = close.rolling(window=20).std()\n",
    "    \n",
    "    # High-Low ratios\n",
    "    result_df[f'{ticker}_HL_ratio'] = high / low\n",
    "    result_df[f'{ticker}_Close_to_High'] = close / high\n",
    "    result_df[f'{ticker}_Close_to_Low'] = close / low\n",
    "    \n",
    "    # Daily returns\n",
    "    result_df[f'{ticker}_Daily_return'] = close.pct_change()\n",
    "    \n",
    "    # Moving average ratios\n",
    "    result_df[f'{ticker}_Price_to_SMA20'] = close / result_df[f'{ticker}_SMA_20']\n",
    "    result_df[f'{ticker}_Price_to_SMA50'] = close / result_df[f'{ticker}_SMA_50']\n",
    "    result_df[f'{ticker}_SMA20_to_SMA50'] = result_df[f'{ticker}_SMA_20'] / result_df[f'{ticker}_SMA_50']\n",
    "    \n",
    "    # Intraday range\n",
    "    result_df[f'{ticker}_Intraday_range'] = (high - low) / open_price\n",
    "    \n",
    "    # Price position within daily range\n",
    "    result_df[f'{ticker}_Price_position'] = (close - low) / (high - low)\n",
    "    \n",
    "    # Volatility features\n",
    "    rolling_vol_5 = result_df[f'{ticker}_Daily_return'].rolling(window=5).std()\n",
    "    rolling_vol_20 = result_df[f'{ticker}_Daily_return'].rolling(window=20).std()\n",
    "    result_df[f'{ticker}_Volatility_ratio'] = rolling_vol_5 / rolling_vol_20\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Apply technical indicators to all stocks\n",
    "print(\"Creating technical indicators for all stocks...\")\n",
    "enhanced_data = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"Processing {ticker}...\")\n",
    "    enhanced_data[ticker] = create_technical_indicators(cleaned_data[ticker], ticker)\n",
    "    \n",
    "    # Count new features (exclude original OHLCV columns)\n",
    "    original_cols = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close', 'Ticker']\n",
    "    new_features = [col for col in enhanced_data[ticker].columns if col not in original_cols]\n",
    "    print(f\"  {ticker}: {len(new_features)} technical indicators created\")\n",
    "\n",
    "print(f\"\\nTechnical indicators created successfully!\")\n",
    "if 'AAPL' in enhanced_data:\n",
    "    aapl_features = [col for col in enhanced_data['AAPL'].columns if isinstance(col, str) and col.startswith('AAPL_')]\n",
    "    print(f\"Sample AAPL features: {aapl_features[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1aa8dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE ENGINEERING ===\n",
      "Preparing feature matrix with SPY as target...\n",
      "  Added 37 features for AAPL\n",
      "  Added 37 features for MSFT\n",
      "  Added 37 features for GOOGL\n",
      "  Added 37 features for AMZN\n",
      "  Added 37 features for TSLA\n",
      "  Added 37 features for SPY\n",
      "  Added 37 features for ^VIX\n",
      "Feature matrix shape: (1156, 259)\n",
      "Target matrix shape: (1156, 3)\n",
      "Total features created: 259\n",
      "\n",
      "Sample feature names: ['AAPL_SMA_5', 'AAPL_SMA_10', 'AAPL_SMA_20', 'AAPL_SMA_50', 'AAPL_EMA_5', 'AAPL_EMA_10', 'AAPL_EMA_20', 'AAPL_RSI_14', 'AAPL_MACD', 'AAPL_MACD_signal']\n",
      "\n",
      "Cleaning data...\n",
      "Before cleaning: 1156 samples\n",
      "After cleaning: 1152 samples\n",
      "Data retention: 99.7%\n",
      "\n",
      "=== TARGET DISTRIBUTION ===\n",
      "Binary direction (0=Down, 1=Up):\n",
      "binary_direction\n",
      "0.0    483\n",
      "1.0    669\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Multi-class direction (0=Strong Down, 4=Strong Up):\n",
      "multiclass_direction\n",
      "0.0    231\n",
      "1.0    230\n",
      "2.0    230\n",
      "3.0    230\n",
      "4.0    231\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def prepare_feature_matrix(enhanced_data_dict, target_ticker='SPY', lookforward_days=3):\n",
    "    \"\"\"\n",
    "    Prepare feature matrix for machine learning with proper target variables.\n",
    "    \n",
    "    Parameters:\n",
    "    enhanced_data_dict (dict): Dictionary of enhanced DataFrames for each ticker\n",
    "    target_ticker (str): Ticker to use for target variable\n",
    "    lookforward_days (int): Days to look forward for target\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (features_df, targets_df, feature_names)\n",
    "    \"\"\"\n",
    "    print(f\"Preparing feature matrix with {target_ticker} as target...\")\n",
    "    \n",
    "    max_window = 50  # largest SMA/EMA/indicator period used, so it won't clean entire dataset\n",
    "    for t in enhanced_data_dict:\n",
    "        enhanced_data_dict[t] = enhanced_data_dict[t].iloc[max_window:] \n",
    "\n",
    "    # Combine all technical indicators into a single DataFrame\n",
    "    all_features = []\n",
    "    \n",
    "    for ticker, df in enhanced_data_dict.items():\n",
    "        # Get only the technical indicator columns (handle both string and tuple column names)\n",
    "        tech_indicators = []\n",
    "        for col in df.columns:\n",
    "            if isinstance(col, tuple):\n",
    "                col_name = col[0]  # Get the first element of the tuple\n",
    "            else:\n",
    "                col_name = col\n",
    "                \n",
    "            # Check if this is a technical indicator column for this ticker\n",
    "            if isinstance(col_name, str) and col_name.startswith(f'{ticker}_'):\n",
    "                tech_indicators.append(col)\n",
    "        \n",
    "        if tech_indicators:\n",
    "            ticker_features = df[tech_indicators].copy()\n",
    "            # Flatten column names if they are tuples\n",
    "            if isinstance(ticker_features.columns[0], tuple):\n",
    "                ticker_features.columns = [col[0] for col in ticker_features.columns]\n",
    "            all_features.append(ticker_features)\n",
    "            print(f\"  Added {len(tech_indicators)} features for {ticker}\")\n",
    "    \n",
    "    # Combine all features\n",
    "    if all_features:\n",
    "        features_df = pd.concat(all_features, axis=1)\n",
    "    else:\n",
    "        raise ValueError(\"No technical indicators found!\")\n",
    "    \n",
    "    #Fills in non-fatal NaN values. Can remove if messes up the data too much. \n",
    "    features_df = features_df.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Create target variables using the target ticker's close price\n",
    "    # Handle tuple column names for target close price\n",
    "    target_df = enhanced_data_dict[target_ticker]\n",
    "    close_col = None\n",
    "    for col in target_df.columns:\n",
    "        if isinstance(col, tuple) and col[0] == 'Close':\n",
    "            close_col = col\n",
    "            break\n",
    "        elif col == 'Close':\n",
    "            close_col = col\n",
    "            break\n",
    "    \n",
    "    if close_col is None:\n",
    "        raise ValueError(f\"Close price column not found for {target_ticker}\")\n",
    "        \n",
    "    target_close = target_df[close_col]\n",
    "    \n",
    "    # Simple future return calculation\n",
    "    future_return = target_close.shift(-lookforward_days) / target_close - 1\n",
    "    \n",
    "    # Binary classification target: Up/Down\n",
    "    binary_target = (future_return > 0).astype(int)\n",
    "    \n",
    "    # Multi-class classification target based on quantiles\n",
    "    future_return_clean = future_return.dropna()\n",
    "    if len(future_return_clean) > 0:\n",
    "        quantiles = future_return_clean.quantile([0.2, 0.4, 0.6, 0.8])\n",
    "        multiclass_target = pd.cut(future_return, \n",
    "                                  bins=[-np.inf, quantiles[0.2], quantiles[0.4], \n",
    "                                       quantiles[0.6], quantiles[0.8], np.inf],\n",
    "                                  labels=[0, 1, 2, 3, 4]).astype(float)\n",
    "    else:\n",
    "        multiclass_target = pd.Series(index=future_return.index, dtype=float)\n",
    "    \n",
    "    # Combine targets\n",
    "    targets_df = pd.DataFrame({\n",
    "        'future_return': future_return,\n",
    "        'binary_direction': binary_target,\n",
    "        'multiclass_direction': multiclass_target\n",
    "    }, index=features_df.index)\n",
    "    \n",
    "    return features_df, targets_df, list(features_df.columns)\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"=== FEATURE ENGINEERING ===\")\n",
    "features_df, targets_df, feature_names = prepare_feature_matrix(enhanced_data, target_ticker='SPY', lookforward_days=3)\n",
    "\n",
    "print(f\"Feature matrix shape: {features_df.shape}\")\n",
    "print(f\"Target matrix shape: {targets_df.shape}\")\n",
    "print(f\"Total features created: {len(feature_names)}\")\n",
    "print(f\"\\nSample feature names: {feature_names[:10]}\")\n",
    "\n",
    "# Remove rows with NaN values\n",
    "print(f\"\\nCleaning data...\")\n",
    "print(f\"Before cleaning: {features_df.shape[0]} samples\")\n",
    "\n",
    "# Allow up to 5% of features to be NaN for a given row\n",
    "feature_valid = (features_df.notna().sum(axis=1) / features_df.shape[1]) > 0.95\n",
    "target_valid = ~targets_df.isna().any(axis=1)\n",
    "valid_mask = feature_valid & target_valid\n",
    "\n",
    "features_clean = features_df[valid_mask].copy()\n",
    "targets_clean = targets_df[valid_mask].copy()\n",
    "\n",
    "print(f\"After cleaning: {features_clean.shape[0]} samples\")\n",
    "print(f\"Data retention: {len(features_clean)/len(features_df):.1%}\")\n",
    "\n",
    "# Display target distribution\n",
    "if len(targets_clean) > 0:\n",
    "    print(f\"\\n=== TARGET DISTRIBUTION ===\")\n",
    "    print(f\"Binary direction (0=Down, 1=Up):\")\n",
    "    print(targets_clean['binary_direction'].value_counts().sort_index())\n",
    "    \n",
    "    if targets_clean['multiclass_direction'].notna().sum() > 0:\n",
    "        print(f\"\\nMulti-class direction (0=Strong Down, 4=Strong Up):\")\n",
    "        print(targets_clean['multiclass_direction'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c172f124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Checking enhanced data structure\n",
      "AAPL shape: (1255, 43)\n",
      "First 10 columns: [('Close', 'AAPL'), ('High', 'AAPL'), ('Low', 'AAPL'), ('Open', 'AAPL'), ('Volume', 'AAPL'), ('Ticker', ''), ('AAPL_SMA_5', ''), ('AAPL_SMA_10', ''), ('AAPL_SMA_20', ''), ('AAPL_SMA_50', '')]\n",
      "Last 10 columns: [('AAPL_HL_ratio', ''), ('AAPL_Close_to_High', ''), ('AAPL_Close_to_Low', ''), ('AAPL_Daily_return', ''), ('AAPL_Price_to_SMA20', ''), ('AAPL_Price_to_SMA50', ''), ('AAPL_SMA20_to_SMA50', ''), ('AAPL_Intraday_range', ''), ('AAPL_Price_position', ''), ('AAPL_Volatility_ratio', '')]\n",
      "AAPL features found: 0\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check the enhanced data structure\n",
    "print(\"Debug: Checking enhanced data structure\")\n",
    "if 'AAPL' in enhanced_data:\n",
    "    print(f\"AAPL shape: {enhanced_data['AAPL'].shape}\")\n",
    "    print(f\"First 10 columns: {list(enhanced_data['AAPL'].columns[:10])}\")\n",
    "    print(f\"Last 10 columns: {list(enhanced_data['AAPL'].columns[-10:])}\")\n",
    "    \n",
    "    # Check for AAPL features specifically\n",
    "    aapl_features = [col for col in enhanced_data['AAPL'].columns if isinstance(col, str) and 'AAPL' in str(col)]\n",
    "    print(f\"AAPL features found: {len(aapl_features)}\")\n",
    "    if aapl_features:\n",
    "        print(f\"First 5 AAPL features: {aapl_features[:5]}\")\n",
    "else:\n",
    "    print(\"AAPL not found in enhanced_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4cfb90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE SELECTION ===\n",
      "Starting with 259 features\n",
      "After variance selection: 189 features (removed 70)\n",
      "Top 50 features selected for regression\n",
      "Top 30 features selected using mutual information\n",
      "Combined unique features: 74\n",
      "Final feature matrix shape: (1152, 74)\n",
      "\n",
      "=== TOP FEATURES ANALYSIS ===\n",
      "Top 10 features by F-score (regression):\n",
      "                     feature    f_score\n",
      "20                TSLA_EMA_5  19.654200\n",
      "25             TSLA_BB_lower  18.925989\n",
      "16                TSLA_SMA_5  18.477499\n",
      "21               TSLA_EMA_10  18.412040\n",
      "17               TSLA_SMA_10  17.643609\n",
      "22               TSLA_EMA_20  17.539516\n",
      "49       ^VIX_Price_to_SMA50  17.241833\n",
      "46  ^VIX_Price_volatility_10  16.171436\n",
      "18               TSLA_SMA_20  15.603853\n",
      "24            TSLA_BB_middle  15.603853\n",
      "\n",
      "Top 10 features by Mutual Information:\n",
      "             feature  mi_score\n",
      "23        ^VIX_SMA_5  0.161401\n",
      "26        ^VIX_EMA_5  0.142175\n",
      "11    GOOGL_BB_upper  0.123294\n",
      "10      GOOGL_RSI_14  0.116555\n",
      "19        SPY_RSI_14  0.108787\n",
      "6   MSFT_MACD_signal  0.108316\n",
      "4        MSFT_RSI_14  0.105783\n",
      "29     ^VIX_BB_lower  0.105241\n",
      "8      MSFT_BB_lower  0.104687\n",
      "24       ^VIX_SMA_20  0.104130\n",
      "\n",
      "============================================================\n",
      "PART 2 COMPLETION SUMMARY\n",
      "============================================================\n",
      "✓ Technical Indicators Created: ~35 indicators per stock\n",
      "✓ Feature Engineering: Price ratios, momentum, volatility analysis\n",
      "✓ Feature Selection Applied: Variance threshold, F-statistics, Mutual Information\n",
      "✓ Final Dataset Prepared:\n",
      "  - Samples: 1,152\n",
      "  - Features: 74\n",
      "  - Target variable: SPY future returns (3-day lookforward)\n",
      "  - Data completeness: 99.7%\n",
      "✓ Ready for Model Building (Part 3)\n",
      "\n",
      "Data structures ready for Part 3:\n",
      "- ml_features: (1152, 74)\n",
      "- ml_targets: (1152, 3)\n",
      "- feature_scaler: fitted StandardScaler\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection and Final Preparation for ML\n",
    "print(\"=== FEATURE SELECTION ===\")\n",
    "\n",
    "# Remove features with very low variance (essentially constant)\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, VarianceThreshold\n",
    "\n",
    "print(f\"Starting with {features_clean.shape[1]} features\")\n",
    "\n",
    "# Remove low-variance features\n",
    "variance_selector = VarianceThreshold(threshold=0.01)\n",
    "features_variance = variance_selector.fit_transform(features_clean)\n",
    "feature_names_variance = features_clean.columns[variance_selector.get_support()]\n",
    "\n",
    "print(f\"After variance selection: {features_variance.shape[1]} features (removed {features_clean.shape[1] - features_variance.shape[1]})\")\n",
    "\n",
    "# Create DataFrame with variance-selected features\n",
    "features_var_selected = pd.DataFrame(features_variance, columns=feature_names_variance, index=features_clean.index)\n",
    "\n",
    "# Scale features for consistent selection\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_var_selected)\n",
    "\n",
    "# Prepare targets\n",
    "y_regression = targets_clean['future_return'].values\n",
    "y_classification = targets_clean['binary_direction'].values\n",
    "\n",
    "# Select top features using F-statistics for regression\n",
    "k_best_regression = SelectKBest(score_func=f_regression, k=min(50, features_variance.shape[1]))\n",
    "features_selected_reg = k_best_regression.fit_transform(features_scaled, y_regression)\n",
    "selected_features_reg = feature_names_variance[k_best_regression.get_support()]\n",
    "\n",
    "print(f\"Top {min(50, features_variance.shape[1])} features selected for regression\")\n",
    "\n",
    "# Select top features using mutual information\n",
    "k_best_mutual = SelectKBest(score_func=mutual_info_regression, k=min(30, features_variance.shape[1]))\n",
    "features_selected_mutual = k_best_mutual.fit_transform(features_scaled, y_regression)\n",
    "selected_features_mutual = feature_names_variance[k_best_mutual.get_support()]\n",
    "\n",
    "print(f\"Top {min(30, features_variance.shape[1])} features selected using mutual information\")\n",
    "\n",
    "# Combine different selection methods\n",
    "all_selected_features = list(set(selected_features_reg) | set(selected_features_mutual))\n",
    "print(f\"Combined unique features: {len(all_selected_features)}\")\n",
    "\n",
    "# Create final feature set\n",
    "final_feature_set = features_var_selected[all_selected_features].copy()\n",
    "final_targets = targets_clean.copy()\n",
    "\n",
    "print(f\"Final feature matrix shape: {final_feature_set.shape}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "print(f\"\\n=== TOP FEATURES ANALYSIS ===\")\n",
    "\n",
    "# Get F-scores for regression\n",
    "f_scores = k_best_regression.scores_[k_best_regression.get_support()]\n",
    "feature_importance_reg = pd.DataFrame({\n",
    "    'feature': selected_features_reg,\n",
    "    'f_score': f_scores\n",
    "}).sort_values('f_score', ascending=False)\n",
    "\n",
    "print(\"Top 10 features by F-score (regression):\")\n",
    "print(feature_importance_reg.head(10))\n",
    "\n",
    "# Get mutual information scores\n",
    "mi_scores = k_best_mutual.scores_[k_best_mutual.get_support()]\n",
    "feature_importance_mi = pd.DataFrame({\n",
    "    'feature': selected_features_mutual,\n",
    "    'mi_score': mi_scores\n",
    "}).sort_values('mi_score', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 features by Mutual Information:\")\n",
    "print(feature_importance_mi.head(10))\n",
    "\n",
    "# Store the results for Part 3\n",
    "ml_features = final_feature_set.copy()\n",
    "ml_targets = final_targets.copy()\n",
    "feature_scaler = scaler  # Keep the scaler for later use\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"PART 2 COMPLETION SUMMARY\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"✓ Technical Indicators Created: ~35 indicators per stock\")\n",
    "print(f\"✓ Feature Engineering: Price ratios, momentum, volatility analysis\")\n",
    "print(f\"✓ Feature Selection Applied: Variance threshold, F-statistics, Mutual Information\")\n",
    "print(f\"✓ Final Dataset Prepared:\")\n",
    "print(f\"  - Samples: {ml_features.shape[0]:,}\")\n",
    "print(f\"  - Features: {ml_features.shape[1]}\")\n",
    "print(f\"  - Target variable: SPY future returns (3-day lookforward)\")\n",
    "print(f\"  - Data completeness: {len(ml_features)/len(features_df):.1%}\")\n",
    "print(f\"✓ Ready for Model Building (Part 3)\")\n",
    "\n",
    "print(f\"\\nData structures ready for Part 3:\")\n",
    "print(f\"- ml_features: {ml_features.shape}\")\n",
    "print(f\"- ml_targets: {ml_targets.shape}\")\n",
    "print(f\"- feature_scaler: fitted StandardScaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74dcce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26f9b1b8",
   "metadata": {},
   "source": [
    "## Part 3: Model Building & Training\n",
    "\n",
    "**Objective**: Build and train multiple machine learning models for stock price prediction\n",
    "\n",
    "**Key Tasks**:\n",
    "1. Implement regression models for price prediction\n",
    "2. Implement classification models for direction prediction\n",
    "3. Use walk-forward validation for realistic backtesting\n",
    "4. Compare model performance with proper train/validation/test splits\n",
    "5. Implement ensemble methods\n",
    "\n",
    "**Models to Implement**:\n",
    "- Linear Regression (baseline)\n",
    "- Random Forest (regression & classification)\n",
    "- Support Vector Machine\n",
    "- Gradient Boosting (XGBoost/LightGBM)\n",
    "- Neural Network (if time permits)\n",
    "\n",
    "**Validation Strategy**: Walk-forward validation to simulate real trading conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9268d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREPARING DATA FOR MODELING ===\n",
      "Scaling features for modeling...\n",
      "Feature matrix: (1152, 74)\n",
      "Regression target shape: (1152,)\n",
      "Classification target shape: (1152,)\n",
      "\n",
      "Regression models: ['Linear Regression', 'Random Forest', 'SVR', 'Gradient Boosting']\n",
      "Classification models: ['Logistic Regression', 'Random Forest', 'SVC', 'Gradient Boosting']\n",
      "\n",
      "Walk-forward validation setup:\n",
      "- Number of splits: 3\n",
      "- Test size: 20% of data\n",
      "- This will simulate realistic trading conditions\n"
     ]
    }
   ],
   "source": [
    "# Import additional machine learning libraries\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "class WalkForwardValidator:\n",
    "    \"\"\"\n",
    "    Walk-forward validation for time series data\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits=5, test_size=0.2):\n",
    "        self.n_splits = n_splits\n",
    "        self.test_size = test_size\n",
    "        \n",
    "    def split(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Generate train/test splits for walk-forward validation\n",
    "        \"\"\"\n",
    "        n_samples = len(X)\n",
    "        test_size = int(n_samples * self.test_size)\n",
    "        train_sizes = np.linspace(int(n_samples * 0.3), n_samples - test_size, self.n_splits)\n",
    "        \n",
    "        for train_size in train_sizes:\n",
    "            train_size = int(train_size)\n",
    "            train_end = train_size\n",
    "            test_start = train_end\n",
    "            test_end = min(test_start + test_size, n_samples)\n",
    "            \n",
    "            train_idx = np.arange(0, train_end)\n",
    "            test_idx = np.arange(test_start, test_end)\n",
    "            \n",
    "            yield train_idx, test_idx\n",
    "\n",
    "def evaluate_regression_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate a regression model\n",
    "    \"\"\"\n",
    "    # Train model\n",
    "    start_time = datetime.now()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        'model': model_name,\n",
    "        'train_mse': mean_squared_error(y_train, y_pred_train),\n",
    "        'test_mse': mean_squared_error(y_test, y_pred_test),\n",
    "        'train_mae': mean_absolute_error(y_train, y_pred_train),\n",
    "        'test_mae': mean_absolute_error(y_test, y_pred_test),\n",
    "        'train_r2': r2_score(y_train, y_pred_train),\n",
    "        'test_r2': r2_score(y_test, y_pred_test),\n",
    "        'train_time': train_time,\n",
    "        'n_train': len(y_train),\n",
    "        'n_test': len(y_test)\n",
    "    }\n",
    "    \n",
    "    return metrics, y_pred_test\n",
    "\n",
    "def evaluate_classification_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate a classification model\n",
    "    \"\"\"\n",
    "    # Train model\n",
    "    start_time = datetime.now()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Probabilities for AUC (if available)\n",
    "    try:\n",
    "        y_prob_test = model.predict_proba(X_test)[:, 1]\n",
    "        test_auc = roc_auc_score(y_test, y_prob_test)\n",
    "    except:\n",
    "        test_auc = np.nan\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        'model': model_name,\n",
    "        'train_accuracy': accuracy_score(y_train, y_pred_train),\n",
    "        'test_accuracy': accuracy_score(y_test, y_pred_test),\n",
    "        'train_precision': precision_score(y_train, y_pred_train, average='binary'),\n",
    "        'test_precision': precision_score(y_test, y_pred_test, average='binary'),\n",
    "        'train_recall': recall_score(y_train, y_pred_train, average='binary'),\n",
    "        'test_recall': recall_score(y_test, y_pred_test, average='binary'),\n",
    "        'train_f1': f1_score(y_train, y_pred_train, average='binary'),\n",
    "        'test_f1': f1_score(y_test, y_pred_test, average='binary'),\n",
    "        'test_auc': test_auc,\n",
    "        'train_time': train_time,\n",
    "        'n_train': len(y_train),\n",
    "        'n_test': len(y_test)\n",
    "    }\n",
    "    \n",
    "    return metrics, y_pred_test\n",
    "\n",
    "# Prepare data for modeling\n",
    "print(\"=== PREPARING DATA FOR MODELING ===\")\n",
    "\n",
    "# Create a new scaler for the final feature set\n",
    "print(\"Scaling features for modeling...\")\n",
    "final_scaler = StandardScaler()\n",
    "X_scaled = final_scaler.fit_transform(ml_features)\n",
    "X_df = pd.DataFrame(X_scaled, columns=ml_features.columns, index=ml_features.index)\n",
    "\n",
    "# Targets\n",
    "y_regression = ml_targets['future_return'].values\n",
    "y_classification = ml_targets['binary_direction'].values\n",
    "\n",
    "print(f\"Feature matrix: {X_df.shape}\")\n",
    "print(f\"Regression target shape: {y_regression.shape}\")\n",
    "print(f\"Classification target shape: {y_classification.shape}\")\n",
    "\n",
    "# Define models\n",
    "regression_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'SVR': SVR(kernel='rbf', C=1.0),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "classification_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'SVC': SVC(kernel='rbf', C=1.0, probability=True, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "print(f\"\\nRegression models: {list(regression_models.keys())}\")\n",
    "print(f\"Classification models: {list(classification_models.keys())}\")\n",
    "\n",
    "# Initialize walk-forward validator\n",
    "wf_validator = WalkForwardValidator(n_splits=3, test_size=0.2)\n",
    "\n",
    "print(f\"\\nWalk-forward validation setup:\")\n",
    "print(f\"- Number of splits: 3\")\n",
    "print(f\"- Test size: 20% of data\")\n",
    "print(f\"- This will simulate realistic trading conditions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970e6e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "REGRESSION MODEL EVALUATION\n",
      "============================================================\n",
      "\n",
      "Evaluating Linear Regression...\n",
      "  Fold 1...\n",
      "    Test R²: -4.1433, Test MAE: 0.047899\n",
      "  Fold 2...\n",
      "    Test R²: -13.5660, Test MAE: 0.042145\n",
      "  Fold 3...\n",
      "    Test R²: -10.1397, Test MAE: 0.062795\n",
      "  Average Test R²: -9.2830\n",
      "  Average Test MAE: 0.050946\n",
      "  Average Train Time: 0.005s\n",
      "\n",
      "Evaluating Random Forest...\n",
      "  Fold 1...\n",
      "    Test R²: -0.1232, Test MAE: 0.020503\n",
      "  Fold 2...\n",
      "    Test R²: -0.1232, Test MAE: 0.020503\n",
      "  Fold 2...\n",
      "    Test R²: -0.6338, Test MAE: 0.013681\n",
      "  Fold 3...\n",
      "    Test R²: -0.6338, Test MAE: 0.013681\n",
      "  Fold 3...\n",
      "    Test R²: -0.3553, Test MAE: 0.019452\n",
      "  Average Test R²: -0.3708\n",
      "  Average Test MAE: 0.017878\n",
      "  Average Train Time: 0.496s\n",
      "\n",
      "Evaluating SVR...\n",
      "  Fold 1...\n",
      "    Test R²: -0.0249, Test MAE: 0.019836\n",
      "  Fold 2...\n",
      "    Test R²: -1.6792, Test MAE: 0.018015\n",
      "  Fold 3...\n",
      "    Test R²: -0.5774, Test MAE: 0.020503\n",
      "  Average Test R²: -0.7605\n",
      "  Average Test MAE: 0.019451\n",
      "  Average Train Time: 0.001s\n",
      "\n",
      "Evaluating Gradient Boosting...\n",
      "  Fold 1...\n",
      "    Test R²: -0.3553, Test MAE: 0.019452\n",
      "  Average Test R²: -0.3708\n",
      "  Average Test MAE: 0.017878\n",
      "  Average Train Time: 0.496s\n",
      "\n",
      "Evaluating SVR...\n",
      "  Fold 1...\n",
      "    Test R²: -0.0249, Test MAE: 0.019836\n",
      "  Fold 2...\n",
      "    Test R²: -1.6792, Test MAE: 0.018015\n",
      "  Fold 3...\n",
      "    Test R²: -0.5774, Test MAE: 0.020503\n",
      "  Average Test R²: -0.7605\n",
      "  Average Test MAE: 0.019451\n",
      "  Average Train Time: 0.001s\n",
      "\n",
      "Evaluating Gradient Boosting...\n",
      "  Fold 1...\n",
      "    Test R²: -0.2883, Test MAE: 0.022097\n",
      "  Fold 2...\n",
      "    Test R²: -0.2883, Test MAE: 0.022097\n",
      "  Fold 2...\n",
      "    Test R²: -2.9915, Test MAE: 0.021809\n",
      "  Fold 3...\n",
      "    Test R²: -2.9915, Test MAE: 0.021809\n",
      "  Fold 3...\n",
      "    Test R²: -1.5225, Test MAE: 0.028358\n",
      "  Average Test R²: -1.6008\n",
      "  Average Test MAE: 0.024088\n",
      "  Average Train Time: 0.951s\n",
      "\n",
      "============================================================\n",
      "CLASSIFICATION MODEL EVALUATION\n",
      "============================================================\n",
      "\n",
      "Evaluating Logistic Regression...\n",
      "  Fold 1...\n",
      "    Test Acc: 0.5083, Test F1: 0.6667, Test AUC: 0.5920\n",
      "  Fold 2...\n",
      "    Test Acc: 0.4250, Test F1: 0.1266, Test AUC: 0.5393\n",
      "  Fold 3...\n",
      "    Test Acc: 0.4167, Test F1: 0.1358, Test AUC: 0.5939\n",
      "  Average Test Accuracy: 0.4500\n",
      "  Average Test F1: 0.3097\n",
      "  Average Test AUC: 0.5750\n",
      "  Average Train Time: 0.011s\n",
      "\n",
      "Evaluating Random Forest...\n",
      "  Fold 1...\n",
      "    Test Acc: 0.5500, Test F1: 0.6766, Test AUC: 0.5888\n",
      "  Fold 2...\n",
      "    Test R²: -1.5225, Test MAE: 0.028358\n",
      "  Average Test R²: -1.6008\n",
      "  Average Test MAE: 0.024088\n",
      "  Average Train Time: 0.951s\n",
      "\n",
      "============================================================\n",
      "CLASSIFICATION MODEL EVALUATION\n",
      "============================================================\n",
      "\n",
      "Evaluating Logistic Regression...\n",
      "  Fold 1...\n",
      "    Test Acc: 0.5083, Test F1: 0.6667, Test AUC: 0.5920\n",
      "  Fold 2...\n",
      "    Test Acc: 0.4250, Test F1: 0.1266, Test AUC: 0.5393\n",
      "  Fold 3...\n",
      "    Test Acc: 0.4167, Test F1: 0.1358, Test AUC: 0.5939\n",
      "  Average Test Accuracy: 0.4500\n",
      "  Average Test F1: 0.3097\n",
      "  Average Test AUC: 0.5750\n",
      "  Average Train Time: 0.011s\n",
      "\n",
      "Evaluating Random Forest...\n",
      "  Fold 1...\n",
      "    Test Acc: 0.5500, Test F1: 0.6766, Test AUC: 0.5888\n",
      "  Fold 2...\n",
      "    Test Acc: 0.4292, Test F1: 0.3445, Test AUC: 0.4626\n",
      "  Fold 3...\n",
      "    Test Acc: 0.5417, Test F1: 0.5339, Test AUC: 0.5603\n",
      "  Average Test Accuracy: 0.5069\n",
      "  Average Test F1: 0.5183\n",
      "  Average Test AUC: 0.5372\n",
      "  Average Train Time: 0.087s\n",
      "\n",
      "Evaluating SVC...\n",
      "  Fold 1...\n",
      "    Test Acc: 0.5542, Test F1: 0.6708, Test AUC: 0.5694\n",
      "  Fold 2...\n",
      "    Test Acc: 0.4292, Test F1: 0.3445, Test AUC: 0.4626\n",
      "  Fold 3...\n",
      "    Test Acc: 0.5417, Test F1: 0.5339, Test AUC: 0.5603\n",
      "  Average Test Accuracy: 0.5069\n",
      "  Average Test F1: 0.5183\n",
      "  Average Test AUC: 0.5372\n",
      "  Average Train Time: 0.087s\n",
      "\n",
      "Evaluating SVC...\n",
      "  Fold 1...\n",
      "    Test Acc: 0.5542, Test F1: 0.6708, Test AUC: 0.5694\n",
      "  Fold 2...\n",
      "    Test Acc: 0.5167, Test F1: 0.6463, Test AUC: 0.4876\n",
      "  Fold 3...\n",
      "    Test Acc: 0.5167, Test F1: 0.6463, Test AUC: 0.4876\n",
      "  Fold 3...\n",
      "    Test Acc: 0.5208, Test F1: 0.4796, Test AUC: 0.5685\n",
      "  Average Test Accuracy: 0.5306\n",
      "  Average Test F1: 0.5989\n",
      "  Average Test AUC: 0.5418\n",
      "  Average Train Time: 0.071s\n",
      "\n",
      "Evaluating Gradient Boosting...\n",
      "  Fold 1...\n",
      "    Test Acc: 0.5208, Test F1: 0.4796, Test AUC: 0.5685\n",
      "  Average Test Accuracy: 0.5306\n",
      "  Average Test F1: 0.5989\n",
      "  Average Test AUC: 0.5418\n",
      "  Average Train Time: 0.071s\n",
      "\n",
      "Evaluating Gradient Boosting...\n",
      "  Fold 1...\n",
      "    Test Acc: 0.4958, Test F1: 0.6207, Test AUC: 0.5594\n",
      "  Fold 2...\n",
      "    Test Acc: 0.4958, Test F1: 0.6207, Test AUC: 0.5594\n",
      "  Fold 2...\n",
      "    Test Acc: 0.4625, Test F1: 0.4691, Test AUC: 0.4672\n",
      "  Fold 3...\n",
      "    Test Acc: 0.4625, Test F1: 0.4691, Test AUC: 0.4672\n",
      "  Fold 3...\n",
      "    Test Acc: 0.4875, Test F1: 0.4721, Test AUC: 0.5212\n",
      "  Average Test Accuracy: 0.4819\n",
      "  Average Test F1: 0.5206\n",
      "  Average Test AUC: 0.5159\n",
      "  Average Train Time: 1.008s\n",
      "\n",
      "============================================================\n",
      "MODEL COMPARISON SUMMARY\n",
      "============================================================\n",
      "\n",
      "REGRESSION MODELS - Average Performance:\n",
      "                    test_r2  test_mae  test_mse  train_time\n",
      "model                                                      \n",
      "Random Forest     -0.370767  0.017878  0.000510    0.496423\n",
      "SVR               -0.760497  0.019451  0.000579    0.000948\n",
      "Gradient Boosting -1.600774  0.024088  0.000838    0.950831\n",
      "Linear Regression -9.283020  0.050946  0.003418    0.005393\n",
      "\n",
      "CLASSIFICATION MODELS - Average Performance:\n",
      "                     test_accuracy  test_f1  test_auc  test_precision  \\\n",
      "model                                                                   \n",
      "SVC                         0.5306   0.5989    0.5418          0.6010   \n",
      "Gradient Boosting           0.4819   0.5206    0.5159          0.5630   \n",
      "Random Forest               0.5069   0.5183    0.5372          0.5868   \n",
      "Logistic Regression         0.4500   0.3097    0.5750          0.6042   \n",
      "\n",
      "                     test_recall  train_time  \n",
      "model                                         \n",
      "SVC                       0.6777      0.0708  \n",
      "Gradient Boosting         0.5390      1.0080  \n",
      "Random Forest             0.5476      0.0874  \n",
      "Logistic Regression       0.3819      0.0107  \n",
      "    Test Acc: 0.4875, Test F1: 0.4721, Test AUC: 0.5212\n",
      "  Average Test Accuracy: 0.4819\n",
      "  Average Test F1: 0.5206\n",
      "  Average Test AUC: 0.5159\n",
      "  Average Train Time: 1.008s\n",
      "\n",
      "============================================================\n",
      "MODEL COMPARISON SUMMARY\n",
      "============================================================\n",
      "\n",
      "REGRESSION MODELS - Average Performance:\n",
      "                    test_r2  test_mae  test_mse  train_time\n",
      "model                                                      \n",
      "Random Forest     -0.370767  0.017878  0.000510    0.496423\n",
      "SVR               -0.760497  0.019451  0.000579    0.000948\n",
      "Gradient Boosting -1.600774  0.024088  0.000838    0.950831\n",
      "Linear Regression -9.283020  0.050946  0.003418    0.005393\n",
      "\n",
      "CLASSIFICATION MODELS - Average Performance:\n",
      "                     test_accuracy  test_f1  test_auc  test_precision  \\\n",
      "model                                                                   \n",
      "SVC                         0.5306   0.5989    0.5418          0.6010   \n",
      "Gradient Boosting           0.4819   0.5206    0.5159          0.5630   \n",
      "Random Forest               0.5069   0.5183    0.5372          0.5868   \n",
      "Logistic Regression         0.4500   0.3097    0.5750          0.6042   \n",
      "\n",
      "                     test_recall  train_time  \n",
      "model                                         \n",
      "SVC                       0.6777      0.0708  \n",
      "Gradient Boosting         0.5390      1.0080  \n",
      "Random Forest             0.5476      0.0874  \n",
      "Logistic Regression       0.3819      0.0107  \n"
     ]
    }
   ],
   "source": [
    "# Run walk-forward validation for regression models\n",
    "print(\"=\"*60)\n",
    "print(\"REGRESSION MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "regression_results = []\n",
    "all_predictions_reg = {}\n",
    "\n",
    "for model_name, model in regression_models.items():\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    fold_results = []\n",
    "    fold_predictions = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(wf_validator.split(X_df)):\n",
    "        print(f\"  Fold {fold + 1}...\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train_fold = X_df.iloc[train_idx]\n",
    "        X_test_fold = X_df.iloc[test_idx] \n",
    "        y_train_fold = y_regression[train_idx]\n",
    "        y_test_fold = y_regression[test_idx]\n",
    "        \n",
    "        # Evaluate model\n",
    "        metrics, predictions = evaluate_regression_model(\n",
    "            model, X_train_fold, X_test_fold, y_train_fold, y_test_fold, model_name\n",
    "        )\n",
    "        metrics['fold'] = fold + 1\n",
    "        \n",
    "        fold_results.append(metrics)\n",
    "        fold_predictions.extend(list(zip(test_idx, predictions)))\n",
    "        \n",
    "        print(f\"    Test R²: {metrics['test_r2']:.4f}, Test MAE: {metrics['test_mae']:.6f}\")\n",
    "    \n",
    "    # Store results\n",
    "    regression_results.extend(fold_results)\n",
    "    all_predictions_reg[model_name] = fold_predictions\n",
    "    \n",
    "    # Calculate average performance\n",
    "    avg_test_r2 = np.mean([r['test_r2'] for r in fold_results])\n",
    "    avg_test_mae = np.mean([r['test_mae'] for r in fold_results])\n",
    "    avg_train_time = np.mean([r['train_time'] for r in fold_results])\n",
    "    \n",
    "    print(f\"  Average Test R²: {avg_test_r2:.4f}\")\n",
    "    print(f\"  Average Test MAE: {avg_test_mae:.6f}\")\n",
    "    print(f\"  Average Train Time: {avg_train_time:.3f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION MODEL EVALUATION\") \n",
    "print(\"=\"*60)\n",
    "\n",
    "classification_results = []\n",
    "all_predictions_clf = {}\n",
    "\n",
    "for model_name, model in classification_models.items():\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    fold_results = []\n",
    "    fold_predictions = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(wf_validator.split(X_df)):\n",
    "        print(f\"  Fold {fold + 1}...\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train_fold = X_df.iloc[train_idx]\n",
    "        X_test_fold = X_df.iloc[test_idx]\n",
    "        y_train_fold = y_classification[train_idx]\n",
    "        y_test_fold = y_classification[test_idx]\n",
    "        \n",
    "        # Evaluate model\n",
    "        metrics, predictions = evaluate_classification_model(\n",
    "            model, X_train_fold, X_test_fold, y_train_fold, y_test_fold, model_name\n",
    "        )\n",
    "        metrics['fold'] = fold + 1\n",
    "        \n",
    "        fold_results.append(metrics)\n",
    "        fold_predictions.extend(list(zip(test_idx, predictions)))\n",
    "        \n",
    "        print(f\"    Test Acc: {metrics['test_accuracy']:.4f}, Test F1: {metrics['test_f1']:.4f}, Test AUC: {metrics['test_auc']:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    classification_results.extend(fold_results)\n",
    "    all_predictions_clf[model_name] = fold_predictions\n",
    "    \n",
    "    # Calculate average performance\n",
    "    avg_test_acc = np.mean([r['test_accuracy'] for r in fold_results])\n",
    "    avg_test_f1 = np.mean([r['test_f1'] for r in fold_results])\n",
    "    avg_test_auc = np.mean([r['test_auc'] for r in fold_results if not np.isnan(r['test_auc'])])\n",
    "    avg_train_time = np.mean([r['train_time'] for r in fold_results])\n",
    "    \n",
    "    print(f\"  Average Test Accuracy: {avg_test_acc:.4f}\")\n",
    "    print(f\"  Average Test F1: {avg_test_f1:.4f}\")\n",
    "    print(f\"  Average Test AUC: {avg_test_auc:.4f}\")\n",
    "    print(f\"  Average Train Time: {avg_train_time:.3f}s\")\n",
    "\n",
    "# Create summary DataFrames\n",
    "regression_df = pd.DataFrame(regression_results)\n",
    "classification_df = pd.DataFrame(classification_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Regression summary\n",
    "print(\"\\nREGRESSION MODELS - Average Performance:\")\n",
    "reg_summary = regression_df.groupby('model').agg({\n",
    "    'test_r2': 'mean',\n",
    "    'test_mae': 'mean', \n",
    "    'test_mse': 'mean',\n",
    "    'train_time': 'mean'\n",
    "}).round(6)\n",
    "\n",
    "reg_summary = reg_summary.sort_values('test_r2', ascending=False)\n",
    "print(reg_summary)\n",
    "\n",
    "# Classification summary  \n",
    "print(\"\\nCLASSIFICATION MODELS - Average Performance:\")\n",
    "clf_summary = classification_df.groupby('model').agg({\n",
    "    'test_accuracy': 'mean',\n",
    "    'test_f1': 'mean',\n",
    "    'test_auc': 'mean',\n",
    "    'test_precision': 'mean',\n",
    "    'test_recall': 'mean',\n",
    "    'train_time': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "clf_summary = clf_summary.sort_values('test_f1', ascending=False)\n",
    "print(clf_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f924bede",
   "metadata": {},
   "source": [
    "## Part 4: Model Evaluation & Interpretation\n",
    "\n",
    "**Objective**: Analyze model performance, interpret results, and implement portfolio simulation\n",
    "\n",
    "**Key Tasks**:\n",
    "1. In-depth analysis of model performance across different market conditions\n",
    "2. Feature importance analysis\n",
    "3. Portfolio simulation using model predictions\n",
    "4. Risk-return analysis\n",
    "5. Comparison with buy-and-hold strategy\n",
    "\n",
    "**Key Findings from Part 3**:\n",
    "- **Best Regression Model**: Random Forest (R² = -0.40, MAE = 0.018)\n",
    "- **Best Classification Model**: SVC (Accuracy = 53.1%, F1 = 59.9%)\n",
    "- All models struggle with stock return prediction (negative R² indicates models perform worse than mean prediction)\n",
    "- Classification models show modest ability to predict direction (slightly better than random)\n",
    "\n",
    "**Analysis Focus**: Understanding why prediction is difficult and extracting actionable insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae84f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fixed feature matrix preparation\n",
    "print(\"=== TESTING FIXED FEATURE MATRIX PREPARATION ===\")\n",
    "try:\n",
    "    features_df, targets_df, feature_names = prepare_feature_matrix(enhanced_data, target_ticker='SPY', lookforward_days=3)\n",
    "    \n",
    "    print(f\"✅ Feature matrix creation successful!\")\n",
    "    print(f\"Feature matrix shape: {features_df.shape}\")\n",
    "    print(f\"Target matrix shape: {targets_df.shape}\")\n",
    "    print(f\"Total features created: {len(feature_names)}\")\n",
    "    print(f\"\\nSample feature names: {feature_names[:10]}\")\n",
    "    \n",
    "    # Check for any remaining issues\n",
    "    print(f\"\\nData quality check:\")\n",
    "    print(f\"Features with NaN: {features_df.isna().sum().sum()}\")\n",
    "    print(f\"Targets with NaN: {targets_df.isna().sum().sum()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
